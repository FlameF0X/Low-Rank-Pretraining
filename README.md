# Low-Rank-Pretraining
LoRPt (Low-Rank Pretraining) is a novel technique that applies LoRA-style low-rank matrix factorization directly to model pretraining, enabling dramatically reduced memory consumption and faster training times for large language models.
